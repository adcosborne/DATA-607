---
title: "Most Valued Data Science Skills"
author: "Group 7"
date: "March 23, 2018"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Data 607: Project 3 

> W. Edwards Deming said, "In God we trust, all others must bring data." Please use data to answer the question, "Which are the most valued data science skills?" Consider your work as an exploration; there is not necessarily a "right answer."

---------------

### Active Members

* AHM Shahparan
* Alejandro Osborne 
* Brian Liles
* Harpeet Shoker
* Sherranette Tinapunan

---------------

### Files

* List of 100 job URLs: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/100Jobs-Shahparan-Version2.csv 

* Job posting text files (scrapped form the web): <br/>
https://github.com/Shetura36/Data-607-Assignments/tree/master/Project3/Jobposting-Archive

* List of term substitution: <br/>
https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/TermsSubstitution_Shahparan-Version5.csv

* List of words removed: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/RemoveTerms-Score20andHigher-Version3.csv

* Term Frequency output: <br/>
https://github.com/Shetura36/Data-607-Assignments/blob/master/Project3/TermFrequency_adjusted_10.csv

* Term Selection file: <br/>
https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/Selected_Terms_version1.csv


---------------

### Load libraries

```{r warning=FALSE, message=FALSE}
library(rvest)
library(knitr)
library(tm)
library(stringr)
library(dplyr)
library(ggplot2)
```

---------------

## Web scraping

### Load job posting URLs

The job URL file contains 100 entries. It has three columns: row_id, job_url, and selector. 
The `selector` column store the selector item as determined by `selectorgadget` tool.

```{r}
#setwd("C:/Users/stina/Documents/GitHub/Data-607-Assignments/Project3")

url_source <- 
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/100Jobs-Shahparan-Version2.csv"

#load the list of URLs
url_listing <- data.frame(read.csv(url_source, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL))
```

### Preview of URL list
```{r echo=FALSE}
row.names(url_listing) <- NULL
kable(url_listing[12:18,], format="markdown")
```

---------------

### Extract and save the job posting

While manually stepping through each URL to confirm that the selector grabs data from each page, I noticed that some of the URLs started to expire. In addition, there was a post in the class slack channel about being banned from a job site. So while going through the the step by step check, a line of code saved each extracted data to a text file just in case the posting expires or access gets denied. However, when an attempt was made to run the entire for loop to go through the 100 URLs to automatically build the dataset of job postings, Monster.com had already blocked access to the site. 

Most of the job postings are from Monster.com and a few are from Indeed.com.  

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics("./images/Monster.com-Forbidden Access-3.23.2018-snippet.png")
```

A team member identified a fix to both issues mentioned above. 

The try-catch block allows the loop to continue onto the next url when a page expires and returns a 404 error. 

To prevent access from being denied, a delay of 10 seconds was added with every retrieval. The code below has been updated with these fixes, and it can slowly build the list of job postings by looping through each url. 

This portion of the code has been commented out because this will take at least around 16 to 20 minutes to rebuild the job posting repository. We already have the job postings archived, and there is no need to re-run this code every time. 

```{r}
#setwd("./JobpostingText")

#for (i in 1:nrow(url_listing)){
#  delayedAssign("do.next", {next})
#  tryCatch(
#    read_html(url_listing$job_url[i]) %>% 
#      html_nodes(url_listing$selector[i]) %>% html_text() -> data ,
#    error = function(e){do.next}
#  )
#    filename <- paste("job", i, ".txt")
#    fileConn<-file(filename)
#    writeLines(c(data), fileConn)
#    close(fileConn)
#    Sys.sleep(10)
#

#setwd("../")
#}
```
--------

## Text mining and data clean up

The `tm` library is used to clean up the data and build a term document matrix. 

The code below will grab all 100 job postings under the specified directory. 
```{r}
cname <- "./Jobposting-Archive"
docs <- VCorpus(DirSource(cname))   
#summary(docs) 
```

### Preview the first document

```{r echo=FALSE}
inspect(docs[[1]])
```


## Data clean up

* convert to lowercase
* remove punctuation
* remove numbers
* remove unnecessary white spaces
* substitute terms  (list of terms from a file)
* remove stop words
* remove irrelevant words (list of terms from a file)

For the removal of terms that have no relevance, a pre-processing of the term document matrix was done first so we can investigate the words that are present in the dataset. The term document matrix was adjusted so that if a term is present in a document, it is only counted once. Mentioning the term more than once in a single job posting does not increase its relevance. 

A term frequency list is generated and reviewed by a team member. Only words with scores of 20 or higher were reviewed to determine their relevance. If a term has a score of 20 and we find that it isn't relevant, the term is included in a removal file, which will be used later on to remove words from the final output. 

### Convert to lowercase
```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Remove characters in regular expression 
```{r}
clean.text <- function(text){
  str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')}
docs <- tm::tm_map(docs, clean.text)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Remove punctuation
```{r}
for (j in seq(docs)) {
    docs[[j]]$content <- gsub("[[:punct:][:blank:]]+", " ", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Identify characters that are NOT letters
```{r}
content <- ""
for(i in 1:length(docs)){
  content <- paste(content, paste(as.character(docs[[i]]), collapse = " "))
}
notLetters <- unlist(str_extract_all(content, '[^A-Za-z ]'))
notLetters <- unique(unlist(strsplit(notLetters, " ")))
```

### Characters that are not letters
```{r echo=FALSE}
notLetters
```

### Remove characters that are not letters
```{r}
for (j in seq(docs)) {
  for (i in 1:length(notLetters)){
    docs[[j]]$content <- gsub(notLetters[i], " ", docs[[j]])
  }
}
docs <- tm_map(docs, stripWhitespace)
#inspect(docs[[1]])
```
### Term substitution

This term substitution file was prepared by several team members.
This file serves two purpose: (1) to preserve multi-term keywords like "data-mining", and (2) to group similar terms into a more encompassing category. For example terms like decisions, decisions support, decisions tools are substituted with the term decisions-science. 

Building the terms substitution file is an iterative process that takes a good amount of effort. It requires close investigation of the top words in the term frequency output; manually going through intermediate outputs, searching for similar terms with lower scores, and making judgement calls if a term should be mapped to a more encompassing term. 

#### Preview list of terms to be substituted
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/TermsSubstitution_Shahparan-Version5.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)

kable(head(terms,10), format="markdown")
#kable(terms)
```

#### Substitute terms from terms substitution file
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], 
                      paste(" ",terms$Replace[i], " "), docs[[j]], ignore.case = TRUE)
  }
}
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Remove stop words
```{r}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
#stopwords("english")   
docs <- tm_map(docs, removeWords, stopwords("english"))  
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Removing terms with no relevance

Terms with scores of 20 or higher and were determined to have no relevance were added to a file. The code below retrieves the terms from this file and substitutes these terms with white spaces. 
```{r}
file <-
  "https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/RemoveTerms-Score20andHigher-Version3.csv"

terms <- 
  read.csv(file, header=TRUE, sep=",", stringsAsFactors = FALSE, row.names = NULL)
```

#### Preview list of terms to be removed
```{r}
kable(head(terms,10), format="markdown")
```

#### Remove terms by substituting whitespace
```{r}
for (j in seq(docs))
{
  #docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  for(i in 1:nrow(terms)){
    docs[[j]] <- gsub(terms$Terms[i], "  ", docs[[j]], ignore.case = TRUE)
  }
}
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
#inspect(docs[[1]])
```

### Preview final output of first document
```{r}
inspect(docs[[1]])
```

-------------------

## Create a term document matrix

After doing the data clean up above, below are the number of terms that remained across 100 documents. 
```{r}
tdm <- TermDocumentMatrix(docs) 
tdm  
```

### Adjust term document matrix count

If a term is mentioned at least once in a document, it is only counted once.  Mentioning the term more than once in a single job posting does not increase its relevance.

The code below will update the count to 1 if the count is greater than 1. 
```{r}
#m <- as.matrix(tdms)
m <- as.matrix(tdm)

#adjust the term count so that when a term appears in a document, it is only counted once
for(i in 1:nrow(m)){
  for(j in 1:ncol(m)){
    if(m[i,j]>1){
      m[i,j] <- 1
    }
  }
}
```

### Create the term frequency list

The code below sums the count across all 100 documents for each term. 
This will assign a frequency score for each term. 

If a term has a score of 20, it means that the term was mentioned at least once in 20 different documents.

```{r}
freq <- rowSums(m)
termFreq <- cbind(names(freq),freq)
rownames(termFreq) <- NULL
colnames(termFreq) <- c("term", "frequency")
termFreq <- data.frame(termFreq, stringsAsFactors = FALSE)
termFreq$frequency <- as.numeric(termFreq$frequency)
```

### Terms with frequency scores of 20 and higher
```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- 
  termFreq %>% dplyr::select(term, frequency) %>% dplyr::filter(frequency > 19) %>% dplyr::arrange(desc(frequency))
kable(result,format="markdown")
```

### Write the term frequency to a file
```{r}
result <- 
  termFreq %>% dplyr::select(term, frequency) %>% dplyr::arrange(desc(frequency))
#write.csv(m, file="TermDocumentMatrix_adjusted.csv") 
write.csv(result, file="TermFrequency_adjusted_10.csv")
```

--------------

### Visual Presentation

A team member analyzed the terms with scores of 20 or higher, and assigned the terms into four different categories: technical skills, technology specific skills, general technical areas, and nontechnical skills. 

This data is quite telling as it shows even though hard technical skills are crucial, soft skills are as important. The need for team work and communication are quite high on the list of competencies that are being searched for by recruiting teams.

```{r echo=FALSE}
selectedTerms <- 
  read.csv("https://raw.githubusercontent.com/Shetura36/Data-607-Assignments/master/Project3/Selected_Terms_version1.csv", header=TRUE, sep=",")

technical_skill <- 
  selectedTerms %>% dplyr::filter(Category=="technical-skill") %>% arrange(desc(frequency))
  
technical_area <-
  selectedTerms %>% dplyr::filter(Category=="technical-area") %>% arrange(desc(frequency))

technical_specific <-
  selectedTerms %>% dplyr::filter(Category=="technical-specific-technology") %>% arrange(desc(frequency))

nontechnical <- 
    selectedTerms %>% dplyr::filter(Category=="nontechnical") %>%   arrange(desc(frequency))
```

------

```{r  echo=FALSE}

ggplot(data = technical_specific, aes(x=reorder(term, -frequency), y=frequency)) + 
  geom_bar(stat="identity", width=0.5, color="#1F3552", fill="steelblue", 
           position=position_dodge()) +
    geom_text(aes(label=round(frequency, digits=2)), vjust=1.3, size=3.0, color="white") + 
    ggtitle("") +
    ylab("Occurence in 100 job entries") + xlab("Technology specific skills") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=11)) 
```


```{r  echo=FALSE}

ggplot(data = technical_skill, aes(x=reorder(term, -frequency), y=frequency)) + 
  geom_bar(stat="identity", width=0.5, color="#1F3552", fill="steelblue", 
           position=position_dodge()) +
    geom_text(aes(label=round(frequency, digits=2)), vjust=1.3, size=3.0, color="white") + 
    ggtitle("") +
    ylab("Occurence in 100 job entries") + xlab("Technical skills") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=11)) 
```


```{r  echo=FALSE}

ggplot(data = technical_area, aes(x=reorder(term, -frequency), y=frequency)) + 
  geom_bar(stat="identity", width=0.5, color="#1F3552", fill="steelblue", 
           position=position_dodge()) +
    geom_text(aes(label=round(frequency, digits=2)), vjust=1.3, size=3.0, color="white") + 
    ggtitle("") +
    ylab("Occurence in 100 job entries") + xlab("Techical areas") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=11)) 
```

```{r echo=FALSE}

ggplot(data = nontechnical, aes(x=reorder(term, -frequency), y=frequency)) + 
  geom_bar(stat="identity", width=0.5, color="#1F3552", fill="steelblue", 
           position=position_dodge()) +
    geom_text(aes(label=round(frequency, digits=2)), vjust=1.3, size=3.0, color="white") + 
    ggtitle("") +
    ylab("Occurence in 100 job entries") + xlab("Nontechnical skills") + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1, size=11)) 
```


        
--------

